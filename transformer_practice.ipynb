{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2RlOoB7VX1qK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Self Attention!!\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embed_size, heads):\n",
        "    super(SelfAttention, self).__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.heads = heads\n",
        "    self.head_dim = embed_size // heads\n",
        "\n",
        "    assert (self.head_dim * heads == embed_size), \"Embed size needs to be divisble by heads\"\n",
        "\n",
        "    self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
        "\n",
        "  def forward(self, values, keys, query, mask):\n",
        "    N = query.shape[0]\n",
        "    value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "    # Split embedding into self.head pieces\n",
        "    values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "    keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "    queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "    values = self.values(values)\n",
        "    keys = self.keys(keys)\n",
        "    queries = self.queries(queries)\n",
        "\n",
        "    energy = torch.einsum(\"nqhd,nkhd->nhqk\", queries, keys)\n",
        "\n",
        "    # queries shape: (N, query_len, heads, head_dim)\n",
        "    # keys shape: (N, key_len, heads, heads_dim)\n",
        "    # energy shape: (N, heads, query_len, key_len)\n",
        "\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "    attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
        "\n",
        "    out = torch.einsum(\"nhql, nlhd->nqhd\", attention, values).reshape(\n",
        "        N, query_len, self.heads*self.head_dim)\n",
        "\n",
        "    # attention shape: (N, heads, query_len, key_len)\n",
        "    # values shape: (N, value_len, heads, heads_dim)\n",
        "    # after sinsum (N, query_len, heads, heads_dim) then flatten last two dimensions\n",
        "\n",
        "    out = self.fc_out(out)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "IAFFeA09Y8az"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEncoding(nn.Module):\n",
        "  def __init__(self, embed_size: int, seq_len: int, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.d_model = embed_size\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Create a matrix of shape (Seq_len, d_model)\n",
        "    pe = torch.zeros(seq_len, embed_size)\n",
        "    # Create a vector of shape\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
        "    div_term = torch.exp(torch.arange(0,embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
        "    # Apply the sin to even positions\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "    pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "    self.register_buffer(\"pe\", pe)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "fUJeICUHW9ah"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size: int, seq_len: int, heads: int,  dropout=0.1,):\n",
        "    super().__init__()\n",
        "\n",
        "    self.we = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n",
        "    self.pe = PositionEncoding(embed_size=embed_size, seq_len=seq_len, dropout=dropout)\n",
        "    self.attention = SelfAttention(embed_size=embed_size, heads=heads)\n",
        "    self.fc_layer = nn.Linear(in_features=embed_size, out_features=vocab_size)\n",
        "\n",
        "    self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, token_ids):\n",
        "\n",
        "    word_embeddings = self.we(token_ids)\n",
        "    print(f\"Word Embeddings are:\\n{word_embeddings}\\n\")\n",
        "\n",
        "    pos_embeddings = self.pe(word_embeddings)\n",
        "    print(f\"Positional Embeddings are:\\n{pos_embeddings}\\n\")\n",
        "\n",
        "    mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0))))\n",
        "    mask = mask == 0\n",
        "    print(f\"Mask is:\\n{mask}\\n\")\n",
        "\n",
        "    self_attention_values = self.attention(pos_embeddings,\n",
        "                                           pos_embeddings,\n",
        "                                           pos_embeddings,\n",
        "                                           mask=mask)\n",
        "    print(f\"Self Attention Values are:\\n{self_attention_values}\\n\")\n",
        "\n",
        "    residual_connection_values = pos_embeddings + self_attention_values\n",
        "    print(f\"Residual Connection Values are:\\n{residual_connection_values}\\n\")\n",
        "    fc_layer_output = self.fc_layer(residual_connection_values)\n",
        "    print(f\"FC Layer Output is:\\n{fc_layer_output}\\n\")\n",
        "\n",
        "    return fc_layer_output"
      ],
      "metadata": {
        "id": "QBuw5X9sPvqw"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# params\n",
        "vocab_size = 10\n",
        "embed_size = 8\n",
        "seq_len = 5\n",
        "heads = 2\n",
        "dropout = 0.1\n",
        "\n",
        "# Initialize the model\n",
        "model = DecoderModel(vocab_size=vocab_size, embed_size=embed_size, seq_len=seq_len, heads=heads, dropout=dropout)\n",
        "print(model)\n",
        "\n",
        "batch_size = 1\n",
        "dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len)).squeeze(dim=0)\n",
        "print(f\"Dummy Input is:\\n{dummy_input}\\n\")\n",
        "\n",
        "output = model(dummy_input)\n",
        "print(f\"output is {output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqRjlNhHU__L",
        "outputId": "d5ac8be4-16c9-42ee-9c6e-d966faf13c19"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecoderModel(\n",
            "  (we): Embedding(10, 8)\n",
            "  (pe): PositionEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (attention): SelfAttention(\n",
            "    (values): Linear(in_features=4, out_features=4, bias=False)\n",
            "    (keys): Linear(in_features=4, out_features=4, bias=False)\n",
            "    (queries): Linear(in_features=4, out_features=4, bias=False)\n",
            "    (fc_out): Linear(in_features=8, out_features=8, bias=True)\n",
            "  )\n",
            "  (fc_layer): Linear(in_features=8, out_features=10, bias=True)\n",
            "  (loss): CrossEntropyLoss()\n",
            ")\n",
            "Dummy Input is:\n",
            "tensor([0, 8, 6, 5, 5])\n",
            "\n",
            "Word Embeddings are:\n",
            "tensor([[ 0.9019,  1.1199, -0.8971,  0.0770, -0.7473,  0.4240,  0.1549, -0.0558],\n",
            "        [-0.5997,  0.8358, -0.2280, -0.1980, -0.6875, -2.2745,  1.6513,  1.0456],\n",
            "        [-1.4433,  0.4923,  0.9570, -0.2433,  1.5549, -0.7820,  1.3593,  1.4620],\n",
            "        [ 0.9027, -1.6393, -0.4980,  0.1461, -0.3756,  0.4621,  0.2188,  0.1235],\n",
            "        [ 0.9027, -1.6393, -0.4980,  0.1461, -0.3756,  0.4621,  0.2188,  0.1235]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "Positional Embeddings are:\n",
            "tensor([[[ 1.0021,  0.0000, -0.9968,  1.1967, -0.8304,  1.5823,  0.1721,\n",
            "           1.0491],\n",
            "         [ 0.2687,  1.5290, -0.1425,  0.8856, -0.7528, -1.4162,  0.0000,\n",
            "           2.2729],\n",
            "         [-0.5934,  0.0846,  1.2841,  0.8186,  1.7499,  0.2420,  1.5125,\n",
            "           2.7356],\n",
            "         [ 1.1598, -2.9214, -0.2250,  1.2238, -0.3840,  0.0000,  0.2465,\n",
            "           1.2483],\n",
            "         [ 0.0000, -2.5477, -0.1207,  1.1857, -0.3729,  1.6237,  0.2476,\n",
            "           1.2483]]], grad_fn=<MulBackward0>)\n",
            "\n",
            "Mask is:\n",
            "tensor([[False,  True,  True,  True,  True],\n",
            "        [False, False,  True,  True,  True],\n",
            "        [False, False, False,  True,  True],\n",
            "        [False, False, False, False,  True],\n",
            "        [False, False, False, False, False]])\n",
            "\n",
            "Self Attention Values are:\n",
            "tensor([[[-0.4808,  0.0415,  0.1474,  0.2803,  0.3381,  0.6924,  0.8821,\n",
            "           0.1966],\n",
            "         [-0.5646, -0.1829, -0.0554,  0.2826,  0.4201,  0.6977,  0.7715,\n",
            "           0.0301],\n",
            "         [-0.4691, -0.3397, -0.3323,  0.2422,  0.6206,  0.6856,  0.5137,\n",
            "           0.1177],\n",
            "         [-0.4943, -0.3322, -0.3394,  0.1681,  0.4879,  0.5993,  0.2959,\n",
            "           0.0370],\n",
            "         [-0.4650,  0.0453,  0.1348,  0.1924,  0.2785,  0.6015,  0.7020,\n",
            "           0.1600]]], grad_fn=<ViewBackward0>)\n",
            "\n",
            "Residual Connection Values are:\n",
            "tensor([[[ 0.5213,  0.0415, -0.8494,  1.4770, -0.4923,  2.2747,  1.0542,\n",
            "           1.2458],\n",
            "         [-0.2959,  1.3461, -0.1979,  1.1681, -0.3326, -0.7185,  0.7715,\n",
            "           2.3030],\n",
            "         [-1.0625, -0.2551,  0.9518,  1.0609,  2.3706,  0.9276,  2.0262,\n",
            "           2.8533],\n",
            "         [ 0.6654, -3.2536, -0.5645,  1.3919,  0.1039,  0.5993,  0.5424,\n",
            "           1.2853],\n",
            "         [-0.4650, -2.5023,  0.0141,  1.3782, -0.0944,  2.2252,  0.9496,\n",
            "           1.4083]]], grad_fn=<AddBackward0>)\n",
            "\n",
            "FC Layer Output is:\n",
            "tensor([[[ 0.1134, -0.5669, -1.5471, -0.8110, -1.2535, -0.3557, -0.7711,\n",
            "          -0.2514,  1.1300,  0.7302],\n",
            "         [ 1.0728,  0.5609, -1.0812, -0.0064, -0.4836, -1.0500, -0.7105,\n",
            "           0.0632,  0.0462,  1.1298],\n",
            "         [ 0.5521,  2.1930, -1.0502, -0.6458, -0.6575,  0.3883, -0.7224,\n",
            "          -0.3699,  0.1935,  1.2665],\n",
            "         [ 0.0296,  0.1778,  0.1280,  0.3685, -1.7651, -0.3060,  0.0361,\n",
            "           0.1261,  0.0172, -0.5235],\n",
            "         [ 0.0956,  0.2858, -0.7885, -0.3373, -2.1151,  0.1030, -0.1331,\n",
            "           0.0538,  0.8199,  0.3555]]], grad_fn=<ViewBackward0>)\n",
            "\n",
            "output is tensor([[[ 0.1134, -0.5669, -1.5471, -0.8110, -1.2535, -0.3557, -0.7711,\n",
            "          -0.2514,  1.1300,  0.7302],\n",
            "         [ 1.0728,  0.5609, -1.0812, -0.0064, -0.4836, -1.0500, -0.7105,\n",
            "           0.0632,  0.0462,  1.1298],\n",
            "         [ 0.5521,  2.1930, -1.0502, -0.6458, -0.6575,  0.3883, -0.7224,\n",
            "          -0.3699,  0.1935,  1.2665],\n",
            "         [ 0.0296,  0.1778,  0.1280,  0.3685, -1.7651, -0.3060,  0.0361,\n",
            "           0.1261,  0.0172, -0.5235],\n",
            "         [ 0.0956,  0.2858, -0.7885, -0.3373, -2.1151,  0.1030, -0.1331,\n",
            "           0.0538,  0.8199,  0.3555]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Haf0E_b4eblr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}